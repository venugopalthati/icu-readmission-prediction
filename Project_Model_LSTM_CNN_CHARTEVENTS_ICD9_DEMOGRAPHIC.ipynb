{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2f1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import imblearn\n",
    "\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836937ba",
   "metadata": {},
   "source": [
    "## Load numpy arrays from the path where process icustays program saved those arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c033eb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48854, 48, 59)\n",
      "(48854, 14)\n",
      "(48854, 17)\n",
      "(48854,)\n"
     ]
    }
   ],
   "source": [
    "path = Path('/finalproject/npoutput').expanduser()\n",
    "\n",
    "chart_data_np = np.load(path/'chart_all_data_04272109.npy')\n",
    "demographic_data_np = np.load(path/'demographic_all_data_04272109.npy')\n",
    "icd9_data_np = np.load(path/'icd9_all_data_04272109.npy')\n",
    "y_np = np.load(path/'y_all_04272109.npy')\n",
    "    \n",
    "#chart_data_np = np.load(path/'chart_sample_data_04252037.npy')\n",
    "#demographic_data_np = np.load(path/'demographic_sample_data_04252037.npy')\n",
    "#icd9_data_np = np.load(path/'icd9_sample_data_04252037.npy')\n",
    "#y_np = np.load(path/'y_04252037.npy')\n",
    "\n",
    "\n",
    "print(chart_data_np.shape)\n",
    "print(demographic_data_np.shape)\n",
    "print(icd9_data_np.shape)\n",
    "print(y_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbb137",
   "metadata": {},
   "source": [
    "## Create a new numpy array concatenating chart events, icd9 groups and patinet demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a73d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48854, 48, 90)\n",
      "(48854, 48, 90)\n"
     ]
    }
   ],
   "source": [
    "new_chart_demo_icd9 = np.zeros((chart_data_np.shape[0], chart_data_np.shape[1], chart_data_np.shape[2] + demographic_data_np.shape[1] + icd9_data_np.shape[1]))\n",
    "\n",
    "print(new_chart_demo_icd9.shape)\n",
    "\n",
    "#new_chart_demo[:,:,0:58] = chart_data_np\n",
    "\n",
    "demographic_data_np_new = np.zeros((demographic_data_np.shape[0],48,demographic_data_np.shape[1]))\n",
    "icd9_new = np.zeros((icd9_data_np.shape[0],48,icd9_data_np.shape[1]))\n",
    "\n",
    "for i in range(demographic_data_np.shape[0]):\n",
    "    for j in range(48):\n",
    "        demographic_data_np_new[i,j,:] = demographic_data_np[i, :]\n",
    "    \n",
    "\n",
    "for k in range(icd9_data_np.shape[0]):\n",
    "    for l in range(48):\n",
    "        icd9_new[k,l,:] = icd9_data_np[k, :]\n",
    "        \n",
    "\n",
    "print(new_chart_demo_icd9.shape)\n",
    "new_chart_demo_icd9[:,:,0:59] = chart_data_np\n",
    "new_chart_demo_icd9[:,:,59:73] = demographic_data_np_new\n",
    "new_chart_demo_icd9[:,:,73:91] = icd9_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642b5fc",
   "metadata": {},
   "source": [
    "## Oversample positive icustays as there is data imbalance between positive and negative icustays in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc41f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 39227, 1.0: 9627})\n",
      "(48854, 4320)\n",
      "(48854,)\n",
      "(48854, 48, 59)\n",
      "(78454,)\n",
      "Counter({0.0: 39227, 1.0: 39227})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_np)\n",
    "print(counter)\n",
    "\n",
    "#chart_data_transposed = chart_data_np.transpose(2,0,1).reshape(3,-1)\n",
    "chart_data_np_reshape = new_chart_demo_icd9.reshape(len(new_chart_demo_icd9),-1)\n",
    "print(chart_data_np_reshape.shape)\n",
    "\n",
    "print(y_np.shape)\n",
    "oversample = SMOTE()\n",
    "chart_data_np_reshape, y_np1 = oversample.fit_resample(chart_data_np_reshape, y_np)\n",
    "# summarize the new class distribution\n",
    "chart_data_np_1 = chart_data_np_reshape.reshape(len(chart_data_np_reshape),48,90)\n",
    "counter = Counter(y_np1)\n",
    "print(chart_data_np.shape)\n",
    "print(y_np1.shape)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a907f54",
   "metadata": {},
   "source": [
    "## Convert numpy arrays to pytorch arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd7b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = torch.from_numpy(chart_data_np_1).float()\n",
    "demographic_data = torch.from_numpy(demographic_data_np).float()\n",
    "icd9_data = torch.from_numpy(icd9_data_np).float()\n",
    "y = torch.from_numpy(y_np1).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db942aad",
   "metadata": {},
   "source": [
    "## Create CustomDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18ebe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(chart_data, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44160619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x14e82cd30>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca834aa8",
   "metadata": {},
   "source": [
    "## Collate function to collect batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca20ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    #print(len(data))\n",
    "    x, y = zip(*data)\n",
    "    return torch.stack(x), torch.stack(y).type(torch.LongTensor) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746c3d0",
   "metadata": {},
   "source": [
    "## Split dataset into 80% training, 10% validation and 10% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94320b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 62763\n",
      "Length of val dataset: 7845\n",
      "Length of test dataset: 7846\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, rem_dataset = random_split(dataset, lengths)\n",
    "\n",
    "val_test_split = int(len(rem_dataset)*0.5)\n",
    "\n",
    "val_test_lengths = [val_test_split, len(rem_dataset) - val_test_split]\n",
    "val_dataset, test_dataset = random_split(rem_dataset, val_test_lengths)\n",
    "\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d34468",
   "metadata": {},
   "source": [
    "## Create dataloaders for the train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5909febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, test_dataset, collate_fn):\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09c7a5",
   "metadata": {},
   "source": [
    "## Create Bi-directional LSTM + CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d007db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.lstm = nn.LSTM(input_size=58, hidden_size=58, bidirectional= True, batch_first=True, num_layers=3)\n",
    "        self.lstm = nn.LSTM(input_size=90, hidden_size=90, bidirectional= True, batch_first=True, num_layers=3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstmdropout = nn.Dropout()\n",
    "        \n",
    "        self.cnn1x2 = nn.Conv1d(in_channels=48, out_channels=47, kernel_size=2, padding = 1)\n",
    "        self.cnn1x3 = nn.Conv1d(in_channels=48, out_channels=46, kernel_size=3, padding = 2)\n",
    "        self.cnn1x4 = nn.Conv1d(in_channels=48, out_channels=45, kernel_size=4, padding = 3)\n",
    "        self.maxpool1 = nn.MaxPool1d(3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(3, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool1d(3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(8418, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        #print('batch_size', batch_size)\n",
    "        output,_ = self.lstm(x)\n",
    "        output = self.lstmdropout(self.tanh(output))\n",
    "        #print('after lstm', output.shape)\n",
    "        \n",
    "        output1 = F.relu(self.cnn1x2(output))\n",
    "        #print('after cnn1x2', output1.shape)\n",
    "        output1 = self.maxpool1(output1)\n",
    "        #print('after maxpool1', output1.shape)\n",
    "        output1 = torch.flatten(output1,1,2)\n",
    "        #print('after flatten output1')\n",
    "        #print(output1.shape)\n",
    "        \n",
    "        \n",
    "        output2 = F.relu(self.cnn1x3(output))\n",
    "        #print('after cnn1x3', output2.shape)\n",
    "        output2 = self.maxpool2(output2)\n",
    "        #print('after maxpool2', output2.shape)\n",
    "        output2 = torch.flatten(output2,1,2)\n",
    "        #print('after flatten output2')\n",
    "        #print(output2.shape)\n",
    "        \n",
    "        \n",
    "        output3 = F.relu(self.cnn1x4(output))\n",
    "        #print('after cnn1x4', output3.shape)\n",
    "        output3 = self.maxpool3(output3)\n",
    "        #print('after maxpool3', output3.shape)\n",
    "        output3 = torch.flatten(output3,1,2)\n",
    "        #print('after flatten output3')\n",
    "        #print(output3.shape)\n",
    "        \n",
    "        finaloutput = torch.cat((output1,output2,output3),1)\n",
    "        finaloutput = F.softmax(self.fc3(self.fc2(self.fc1(finaloutput))), dim = 1)\n",
    "        return finaloutput\n",
    "    \n",
    "model = LSTM() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983fa8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(90, 90, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (tanh): Tanh()\n",
       "  (lstmdropout): Dropout(p=0.5, inplace=False)\n",
       "  (cnn1x2): Conv1d(48, 47, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "  (cnn1x3): Conv1d(48, 46, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn1x4): Conv1d(48, 45, kernel_size=(4,), stride=(1,), padding=(3,))\n",
       "  (maxpool1): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  (maxpool2): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  (maxpool3): MaxPool1d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=8418, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ef988",
   "metadata": {},
   "source": [
    "## Print number of parameters used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec48c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700490"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a37ead",
   "metadata": {},
   "source": [
    "## Create Adam optimizer and Cross Entropy Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47ef378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cd5c8",
   "metadata": {},
   "source": [
    "## Model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbe196c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, y in val_loader:\n",
    "        y_hat = model(x)\n",
    "        #y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        #print(y_hat.shape)\n",
    "        for i in range(len(y_hat)):\n",
    "            if y_hat[i][0] == 1:\n",
    "                y_pred.append(0)\n",
    "            else:\n",
    "                y_pred.append(1)\n",
    "        #y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    y_pred_t = torch.Tensor(y_pred)\n",
    "    #print(y_pred_t)\n",
    "    #print(y_true)\n",
    "    acc = accuracy_score(y_true, y_pred_t)\n",
    "    precision = precision_score(y_true, y_pred_t)\n",
    "    recall = recall_score(y_true, y_pred_t)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_t)\n",
    "    \n",
    "    return acc, precision, recall, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6886756",
   "metadata": {},
   "source": [
    "## Unit test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03490d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4935627788400255 0.0 0.0 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pushpalathachiluvari/Library/Python/3.8/lib/python/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "a, p, r, roc_auc = eval_model(model, val_loader)\n",
    "print(a, p, r, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadb709",
   "metadata": {},
   "source": [
    "## Training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5a84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            #print(len(x))\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            #print(y_hat)\n",
    "            #print(y)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        a, p, r, roc_auc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation a: {:.2f}, p:{:.2f}, r: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, a, p, r, roc_auc))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836cb84",
   "metadata": {},
   "source": [
    "## Call the training function for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9226a4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time_nanosec 1651188616548672000\n",
      "Epoch: 1 \t Training Loss: 0.517322\n",
      "Epoch: 1 \t Validation a: 0.81, p:0.92, r: 0.69, roc_auc: 0.81\n",
      "Epoch: 2 \t Training Loss: 0.476446\n",
      "Epoch: 2 \t Validation a: 0.84, p:0.91, r: 0.75, roc_auc: 0.84\n",
      "Epoch: 3 \t Training Loss: 0.474778\n",
      "Epoch: 3 \t Validation a: 0.84, p:0.92, r: 0.74, roc_auc: 0.84\n",
      "Epoch: 4 \t Training Loss: 0.468174\n",
      "Epoch: 4 \t Validation a: 0.83, p:0.93, r: 0.73, roc_auc: 0.83\n",
      "Epoch: 5 \t Training Loss: 0.464624\n",
      "Epoch: 5 \t Validation a: 0.85, p:0.90, r: 0.78, roc_auc: 0.85\n",
      "Epoch: 6 \t Training Loss: 0.463948\n",
      "Epoch: 6 \t Validation a: 0.84, p:0.90, r: 0.76, roc_auc: 0.84\n",
      "Epoch: 7 \t Training Loss: 0.461017\n",
      "Epoch: 7 \t Validation a: 0.85, p:0.92, r: 0.76, roc_auc: 0.85\n",
      "Epoch: 8 \t Training Loss: 0.460717\n",
      "Epoch: 8 \t Validation a: 0.84, p:0.91, r: 0.77, roc_auc: 0.84\n",
      "Epoch: 9 \t Training Loss: 0.458319\n",
      "Epoch: 9 \t Validation a: 0.85, p:0.89, r: 0.80, roc_auc: 0.85\n",
      "Epoch: 10 \t Training Loss: 0.456552\n",
      "Epoch: 10 \t Validation a: 0.84, p:0.92, r: 0.76, roc_auc: 0.84\n",
      "end_time_nanosec 1651190187871716000\n",
      "total time in seconds 1571.323044\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "start_time_nanosec = time.time_ns()\n",
    "print('start_time_nanosec', start_time_nanosec)\n",
    "\n",
    "train(model, train_loader, val_loader, n_epochs)\n",
    "\n",
    "end_time_nanosec = time.time_ns()\n",
    "print('end_time_nanosec', end_time_nanosec)\n",
    "\n",
    "\n",
    "print('total time in seconds', (end_time_nanosec - start_time_nanosec)/1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193406e",
   "metadata": {},
   "source": [
    "## Evaluate the model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49035b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Testing a: 0.852, p:0.921, r: 0.768, roc_auc: 0.852\n"
     ]
    }
   ],
   "source": [
    "a, p, r, roc_auc = eval_model(model, test_loader)\n",
    "print('Epoch: {} \\t Testing a: {:.3f}, p:{:.3f}, r: {:.3f}, roc_auc: {:.3f}'\n",
    "              .format(1, a, p, r, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94bd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
